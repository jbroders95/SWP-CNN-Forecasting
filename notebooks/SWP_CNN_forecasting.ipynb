{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "reach = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_62684\\1926231682.py:12: RuntimeWarning: Mean of empty slice\n",
      "  averaged_depth = np.nanmean(depth_data, axis=(0))\n"
     ]
    }
   ],
   "source": [
    "zarr_array = xr.open_zarr(f'SW_0{reach}_SWP_01_2025-06-13.zarr')\n",
    "\n",
    "depth_data = zarr_array['depth'][::7].values\n",
    "date_data = zarr_array['time'][::7].values\n",
    "zarr_array.close()\n",
    "template_map = np.array([len(depth[~np.isnan(depth)]) for depth in depth_data]).argmax()\n",
    "original_shape = depth_data[template_map].shape\n",
    "\n",
    "\n",
    "mask = np.isnan(depth_data[template_map])\n",
    "mask\n",
    "averaged_depth = np.nanmean(depth_data, axis=(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depth_data_post = depth_data[date_data > np.datetime64('2021-01-01')]\n",
    "date_data_post = date_data[date_data > np.datetime64('2021-01-01')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_masking(depths,mask,scale=1):\n",
    "    random_mask = deepcopy(depths)\n",
    "    for array in depths:\n",
    "        array[mask] = np.random.random(size=np.sum(mask))*scale\n",
    "        array[array==None] = 0\n",
    "        array[array==np.nan] = 0\n",
    "    return random_mask      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_in=8\n",
    "weeks_out=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_1= int(np.ceil(depth_data.shape[1]/2))\n",
    "dim_2= int(np.ceil(depth_data.shape[2]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscale_array_avg(array, factor):\n",
    "    reshaped_array = array.reshape((array.shape[0] // factor, factor, array.shape[1] // factor, factor))\n",
    "    downscaled_array = reshaped_array.mean(axis=(1, 3))\n",
    "    return downscaled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = (dim_1, dim_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the depth array\n",
    "depth_array = depth_data_post[10]\n",
    "normalized_depths = depth_array / np.max(depth_array)\n",
    "# Create an RGB image where depths < 48 appear as red\n",
    "colored_depths = np.zeros((*depth_array.shape, 3))  # Create an RGB image\n",
    "colored_depths[..., 0] = np.where(depth_array < 50, 1, 0)  # Red channel\n",
    "colored_depths[..., 1] = 0  # Green channel\n",
    "colored_depths[..., 2] = np.where(depth_array > 50, 1, 0)  # Blue channel\n",
    "\n",
    "plt.imshow(colored_depths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "reduced_depths = [depth for depth in depth_data if len(depth_data[template_map][~np.isnan(depth_data[template_map])]) == len(depth[~np.isnan(depth)])]\n",
    "reduced_depths = [resize(depth, output_shape, anti_aliasing=True) for depth in reduced_depths]\n",
    "plt.imshow(reduced_depths[10])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dredge_percent_collection(depth,date):\n",
    "    dredge_percent=[]\n",
    "    threshold_deepening = 50\n",
    "    threshold_pre_deepening = 45\n",
    "    combined= [(pos, time) for pos, time in zip(depth, date)]\n",
    "    for item in combined:\n",
    "        if item[1]< np.datetime64('2020-01-01'):\n",
    "            dredge = (item[0]< threshold_pre_deepening).sum()/(~np.isnan(item[0])).sum()\n",
    "            dredge_percent.append([dredge,item[1]])\n",
    "        else:\n",
    "            dredge = (item[0]< threshold_deepening).sum()/(~np.isnan(item[0])).sum()\n",
    "            dredge_percent.append([dredge,item[1]])\n",
    "    return dredge_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dredge_percent = dredge_percent_collection(depth_data,date_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dredge_percent = np.array(dredge_percent,dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the depth array\n",
    "depth_array = depth_data_post[10]\n",
    "normalized_depths = depth_array / np.max(depth_array)\n",
    "# Create an RGB image where depths < 48 appear as red\n",
    "colored_depths = np.zeros((*depth_array.shape, 3))  # Create an RGB image\n",
    "colored_depths[..., 0] = np.where(depth_array < 50, 1, 0)  # Red channel\n",
    "colored_depths[..., 1] = 0  # Green channel\n",
    "colored_depths[..., 2] = np.where(depth_array > 50, 1, 0)  # Blue channel\n",
    "\n",
    "plt.imshow(colored_depths)\n",
    "plt.text(200, 200, f\"Dredge: {dredge_percent[10][0]:.2f}\", color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dredge_percent[:,0]*100,bins=10)\n",
    "plt.xlabel('Prcent of channel area with depth below threshold')\n",
    "plt.title('Percent below threshold histogram')\n",
    "plt.text(10,125,f\"mean:{np.mean(dredge_percent[:,0]):.2f}\")\n",
    "plt.text(10,150,f\"median:{np.median(dredge_percent[:,0]):.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth =80\n",
    "min_depth = 20\n",
    "percent_below = dredge_percent[:,0]\n",
    "scaled_depths = [(arr - min_depth) / (max_depth - min_depth) for arr in reduced_depths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(scaled_depths[10])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reduced_mask = np.isnan(reduced_depths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_depths = random_masking(scaled_depths,reduced_mask,0)\n",
    "masked_depths = random_masking(scaled_depths,reduced_mask,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masked_depths[1])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "percent_below_scaler = scaler.fit(percent_below.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_below_scaled = percent_below_scaler.transform(percent_below.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, input_length, output_length,y_predefined= None,date=None):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_length - output_length + 1):\n",
    "        X.append(data[i:i + input_length:7])\n",
    "        if y_predefined.any()== None:\n",
    "            y.append(np.nanmin(data[i + input_length + output_length-1]))\n",
    "        else:\n",
    "            y.append(y_predefined[i+input_length+output_length-1])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "sequence_depths_X,sequence_depths_y = create_sequences(masked_depths, weeks_in*7, weeks_out*7,percent_below_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X = np.moveaxis(train_X, 1, -1)\n",
    "#test_X = np.moveaxis(test_X, 1, -1)\n",
    "num_samples = len(sequence_depths_X)\n",
    "indices = np.arange(num_samples)\n",
    "\n",
    "train_X, test_X, train_y, test_y,  train_idx, test_idx = train_test_split(sequence_depths_X, sequence_depths_y, indices, test_size=0.2, random_state=42)\n",
    "print(train_X.shape)\n",
    "\n",
    "# Print shapes to verify\n",
    "\n",
    "train_X = np.transpose(train_X,(0,2,3,1))\n",
    "print(train_X.shape)\n",
    "test_X = np.transpose(test_X,(0,2,3,1))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y[:,0].tolist()))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y[:,0].tolist()))\n",
    "\n",
    "train_dataset = train_dataset.batch(64)\n",
    "test_dataset = test_dataset.batch(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples corresponding to the final 6 weeks\n",
    "\n",
    "validation_start_index =  -6\n",
    "\n",
    "# Create validation dataset (only input data)\n",
    "validation_X = sequence_depths_X[validation_start_index:-5]\n",
    "\n",
    "# Print shape to verify\n",
    "validation_X = np.transpose(validation_X,(0,2,3,1))\n",
    "print(\"Validation X shape:\", validation_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates = date_data[train_idx]\n",
    "test_dates = date_data[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Dense, MaxPool3D, Flatten, Input,Dropout,Conv2D,Conv3D, MaxPool2D,GlobalAveragePooling2D,BatchNormalization\n",
    "\n",
    "# Define your model\n",
    "model = Sequential()\n",
    "optimizer= tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=(4, 4), activation='relu', input_shape=( dim_1, dim_2,weeks_in)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(4, 4)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(4, 4), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(4, 4)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(4, 4), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(4, 4)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer=optimizer, loss='mse',metrics = ['mae'])\n",
    "\n",
    "\n",
    "visualkeras.layered_view(model).show() # display using your system viewer\n",
    "\n",
    "visualkeras.layered_view(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,epochs=300, validation_data=(test_dataset),callbacks=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",patience=15, start_from_epoch=0,restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_X,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_X[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the Monte Carlo Dropout function to run stochastic forward passes\n",
    "def monte_carlo_dropout(model, input_data, num_samples=100):\n",
    "    \"\"\"\n",
    "    Run multiple stochastic forward passes with dropout on during inference.\n",
    "    Args:\n",
    "    - model: Trained model.\n",
    "    - input_data: Input data for the model (e.g., a batch of samples).\n",
    "    - num_samples: Number of forward passes to make for uncertainty estimation.\n",
    "    \n",
    "    Returns:\n",
    "    - mean_pred: Mean prediction from multiple passes.\n",
    "    - uncertainty: Standard deviation (uncertainty) from the predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use tf.function for speed-up (graph execution)\n",
    "    @tf.function\n",
    "    def predict_with_dropout(inputs):\n",
    "        # 1 indicates training mode, which means dropout will be active during inference\n",
    "        return model(inputs, training=True)\n",
    "\n",
    "    # Perform multiple stochastic forward passes\n",
    "    predictions = []\n",
    "    for _ in range(num_samples):\n",
    "        pred = predict_with_dropout(input_data)\n",
    "        predictions.append(pred.numpy())  # Convert tensor to numpy array\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate the mean and standard deviation (uncertainty) from the predictions\n",
    "    mean_pred = predictions.mean(axis=0)\n",
    "    uncertainty = predictions.std(axis=0)\n",
    "    \n",
    "    return mean_pred, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'reach_{reach}_CNN.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Monte Carlo Dropout and get both predictions and uncertainty\n",
    "mean_pred, uncertainty = monte_carlo_dropout(model, test_X, num_samples=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(pred_y[50:65]))  # Create an array of indices for the bars\n",
    "\n",
    "dates_np = date_data[test_idx][50:65]\n",
    "dates = [pd.Timestamp(d).to_pydatetime() for d in dates_np]\n",
    "\n",
    "# Convert datetime objects to string in desired format\n",
    "dates_str = [d.strftime('%Y-%m-%d') for d in dates]\n",
    "\n",
    "width = 0.4  # Width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, test_y[50:65,0]*100\n",
    "                ,width, label='Actual (test_y)')\n",
    "rects2 = ax.bar(x + width/2, pred_y[50:65].flatten()\n",
    "                *100\n",
    "                ,width, label='Predicted (pred_y)')\n",
    "\n",
    "errorbar= ax.errorbar(x + width/2, pred_y[50:65].flatten()*100, yerr=uncertainty[50:65].flatten()*100, fmt='o', color='black', label='Uncertainty')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Percent Below Threshold')\n",
    "ax.set_title('Comparison of Actual vs Predicted')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(dates_str, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Optionally, add value labels on top of the bars\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.1f}',  # Format to 2 decimal places\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "# Rotate date labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0,80)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((pred_y[:,0] - test_y.flatten()) ** 2))\n",
    "mae = np.mean(np.abs((pred_y[:,0] - test_y.flatten())))\n",
    "print(f\"RMSE:{rmse.round(3)}\")\n",
    "print(f\"Mae:{mae.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: replace these with your actual data\n",
    "# pred_y = np.array([...])\n",
    "# test_y = np.array([...])\n",
    "# uncertainty = np.array([...])  # same shape as test_y\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot with error bars\n",
    "plt.errorbar(test_y, pred_y.flatten(), yerr=uncertainty.flatten(), fmt='o', ecolor='gray', alpha=0.6)\n",
    "\n",
    "# Reference line: perfect predictions\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"True Values (test_y)\")\n",
    "plt.ylabel(\"Predicted Values (pred_y)\")\n",
    "plt.title(f\"Reach {reach} Scatter Plot\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((test_y[:,0] - pred_y.flatten()) ** 2))\n",
    "mae = np.mean(np.abs((test_y[:,0] - pred_y.flatten())))\n",
    "print(f\"uRMSE: {rmse:.3f}\")\n",
    "print(f\"uMAE: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
