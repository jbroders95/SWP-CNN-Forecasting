{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import cftime\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray  # noqa: F401\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, local_path):\n",
    "    with httpx.Client(timeout=None) as client:\n",
    "        with client.stream(\"GET\", url) as response:\n",
    "            response.raise_for_status()\n",
    "\n",
    "            total = int(response.headers.get(\"Content-Length\", 0))\n",
    "            if total == 0:\n",
    "                print(\"No content length header. Progress bar may not be accurate.\")\n",
    "\n",
    "            with open(local_path, \"wb\") as f, tqdm(\n",
    "                total=total, unit=\"B\", unit_scale=True, desc=\"Downloading\"\n",
    "            ) as pbar:\n",
    "                for chunk in response.iter_bytes(chunk_size=1024 * 1024):  # 1 MB chunks\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "    print(f\" Download complete: {local_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CSAT_NCF_CEMVN_combined_2025-07-01.zip'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path = f\"CSAT_NCF_CEMVN_combined_{datetime.today().strftime('%Y-%m-%d')}.zip\"\n",
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 8.30G/8.30G [54:08<00:00, 2.56MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Download complete: CSAT_NCF_CEMVN_combined_2025-07-01.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "download_file(\n",
    "    \"https://chldata.erdc.dren.mil/thredds/fileServer/csat/CSAT_NCF_CEMVN_combined.zip\",\n",
    "    local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = Path(\"data/original\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(local_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEMVN/SW_01_SWP_01.nc\n",
      "\t112,971 out of 1,119,625 points retained\n",
      "\tn_uLat=1325, n_uLon=844, nTimes=640\n",
      "CEMVN/SW_02_SWP_01.nc\n",
      "\t111,904 out of 1,210,833 points retained\n",
      "\tn_uLat=1027, n_uLon=1178, nTimes=639\n",
      "CEMVN/SW_03_SWP_01.nc\n",
      "\t110,820 out of 1,184,964 points retained\n",
      "\tn_uLat=1146, n_uLon=1034, nTimes=1796\n",
      "CEMVN/SW_04_SWP_01.nc\n",
      "\t110,759 out of 1,035,368 points retained\n",
      "\tn_uLat=1323, n_uLon=782, nTimes=2383\n",
      "CEMVN/SW_05_SWP_01.nc\n",
      "\t121,510 out of 530,136 points retained\n",
      "\tn_uLat=1592, n_uLon=333, nTimes=2969\n",
      "CEMVN/SW_06_SWP_01.nc\n",
      "\t111,279 out of 880,376 points retained\n",
      "\tn_uLat=1393, n_uLon=632, nTimes=2047\n",
      "CEMVN/SW_07_SWP_01.nc\n",
      "\t114,651 out of 1,178,712 points retained\n",
      "\tn_uLat=1284, n_uLon=918, nTimes=1967\n",
      "CEMVN/SW_08_SWP_01.nc\n",
      "\t113,589 out of 1,156,176 points retained\n",
      "\tn_uLat=1302, n_uLon=888, nTimes=1823\n",
      "CEMVN/SW_09_SWP_01.nc\n",
      "\t112,886 out of 1,177,394 points retained\n",
      "\tn_uLat=1277, n_uLon=922, nTimes=1916\n",
      "CEMVN/SW_10_SWP_01.nc\n",
      "\t112,527 out of 1,178,980 points retained\n",
      "\tn_uLat=1265, n_uLon=931, nTimes=1857\n",
      "CEMVN/SW_11_SWP_01.nc\n",
      "\t111,047 out of 1,125,290 points retained\n",
      "\tn_uLat=1310, n_uLon=859, nTimes=1780\n",
      "CEMVN/SW_12_SWP_01.nc\n",
      "\t68,664 out of 672,072 points retained\n",
      "\tn_uLat=984, n_uLon=683, nTimes=1681\n",
      "CEMVN/SW_13_SWP_01.nc\n",
      "\t63,214 out of 86,674 points retained\n",
      "\tn_uLat=1057, n_uLon=82, nTimes=323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_reach_properties(reach_ID=None, returnGeo=False):\n",
    "    \"\"\"\n",
    "    Retrieves select attributes from the National Channel Framework for a given ReachID.\n",
    "    Parameters\n",
    "    ----------\n",
    "    reach_ID : str, optional\n",
    "        The channel reach ID to query (expected format: CEXYZ...)\n",
    "    returnGeo : bool, default False\n",
    "        Whether to include geometry in the returned GeoDataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    geopandas.GeoDataFrame\n",
    "        Contains attributes of the specified reach including:\n",
    "        - channelreachidpk\n",
    "        - sdsfeaturename\n",
    "        - sdsfeaturedescription\n",
    "        - sourceprojection\n",
    "        - channelareaidfk\n",
    "        - depthmaintained\n",
    "        - depthauthorized\n",
    "        - geometry (if returnGeo=True)\n",
    "    Notes\n",
    "    -----\n",
    "    This function queries the ArcGIS REST API for the National Channel Framework.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO Check if reach_ID is a valid format (starts with CEXYZ)\n",
    "    query = (\n",
    "        f\"https://services7.arcgis.com/n1YM8pTrFmm7L4hs/ArcGIS/rest/services/National_Channel_Framework/\"\n",
    "        f\"FeatureServer/2/query?where=channelreachidpk='{reach_ID}'&outFields=channelreachidpk,sdsfeaturename,\"\n",
    "        f\"sdsfeaturedescription,sourceprojection,channelareaidfk,depthmaintained,depthauthorized&returnGeometry={str(returnGeo)}&f=json&token=\"\n",
    "    )\n",
    "\n",
    "    gdf = gpd.read_file(query)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def clean_csat_netcdf(district5, reachID):\n",
    "    \"\"\"\n",
    "    Clean and transform CSAT (Corps Shoaling Analysis Tool) NetCDF data.\n",
    "    This function processes raw CSAT NetCDF files by cleaning the data, converting formats,\n",
    "    and restructuring the data from 2D to 3D arrays. It handles temporal conversions,\n",
    "    extreme value filtering, and proper NetCDF attribute assignments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    district5 : str\n",
    "        The district identifier for the data location\n",
    "    reachID : str\n",
    "        The reach identifier for the specific waterway segment\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function saves the processed NetCDF file to './data/updated/{district5}/{reachID}.nc'\n",
    "    Notes\n",
    "    -----\n",
    "    The function performs the following operations:\n",
    "    - Trims excess points from 2D arrays\n",
    "    - Converts time variables to conventional format\n",
    "    - Replaces extreme float values with NaN\n",
    "    - Converts data structure from 2D to 3D\n",
    "    - Adds proper CF-compliant metadata\n",
    "    - Applies CRS information from reach properties\n",
    "    - Compresses the output file\n",
    "    The output NetCDF file follows CF-1.11 conventions and includes standardized\n",
    "    attributes for depth, time, and spatial coordinates.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> clean_csat_netcdf('CEMVN', 'SW_01_SWP_01')\n",
    "    \"\"\"\n",
    "\n",
    "    sample_path = Path(\n",
    "        rf\"./data/original/{district5}/{reachID}.nc\"\n",
    "    )  # TODO: Update path to match your directory structure, this is where the files downloaded from the CIRP website are stored\n",
    "    print(rf\"{district5}/{reachID}.nc\")\n",
    "\n",
    "    with xr.open_dataset(sample_path, chunks = {\"x\":256,\"y\":256}) as ds:\n",
    "        # --- Trimming 2D array ---\n",
    "        # The excess points can be trimmed by eliminating extreme values in the `points` variable\n",
    "        PointIdx = np.where(ds[\"points\"] >= 0)[0]\n",
    "        print(f\"\\t{len(PointIdx):,} out of {len(ds['points']):,} points retained\")\n",
    "\n",
    "        # ds['elevations'].isel(points=PointIdx)\n",
    "        ds2 = ds.isel(points=PointIdx).copy()\n",
    "\n",
    "    # --- Converting time variable to a conventional format ---\n",
    "    # Use Pandas to parse original time (floats) and create datetime.datetime objects\n",
    "    time_var = ds2[\"time\"].astype(int).astype(str)  # .values.astype(int).astype(str))\n",
    "    time_var = pd.to_datetime(time_var)\n",
    "    time_dt = time_var.to_pydatetime()\n",
    "\n",
    "    # use cftime date2num function to transform datetime.datetime object\n",
    "    time_datum = datetime(1900, 1, 1)\n",
    "    time_units = f\"days since {time_datum.year}-{time_datum.month}-{time_datum.day}\"\n",
    "    time_vals = cftime.date2num(time_dt, time_units)\n",
    "\n",
    "    # --- Replacing extreme float values with nan-values ---\n",
    "    ds2[\"elevations\"] = ds2[\"elevations\"].where(ds2[\"elevations\"] > -9999)\n",
    "\n",
    "    # --- Converting from 2D to 3D ---\n",
    "    uLat, latIndices = np.unique(ds2[\"latitudes\"], return_inverse=True)\n",
    "    uLon, lonIndices = np.unique(ds2[\"longitudes\"], return_inverse=True)\n",
    "    nTimes = len(time_vals)\n",
    "\n",
    "    print(f\"\\tn_uLat={len(uLat)}, n_uLon={len(uLon)}, nTimes={nTimes}\")\n",
    "\n",
    "    elevs_3D = np.full((nTimes, len(uLat), len(uLon)), fill_value=np.nan)\n",
    "\n",
    "    for i in np.arange(nTimes):\n",
    "        elevs_3D[i, latIndices, lonIndices] = ds2[\"elevations\"].isel(time=i).values\n",
    "\n",
    "    ds3 = xr.Dataset(\n",
    "        {\n",
    "            \"depth\": ([\"time\", \"y\", \"x\"], elevs_3D),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": time_vals,\n",
    "            \"y\": ([\"y\"], uLat),\n",
    "            \"x\": ([\"x\"], uLon),\n",
    "            \"reference_time\": pd.Timestamp(\"1900-01-01\"),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Adding global attribute metadata\n",
    "    ds3.attrs[\"Conventions\"] = \"CF-1.11\"\n",
    "    ds3.attrs[\"Title\"] = \"Corps Shoaling Analysis Tool Input Data - Test\"\n",
    "    ds3.attrs[\"description\"] = (\n",
    "        \"This file collates the bathymetry data provided from the USACE eHydro program.  Data is collated by reach in a given USACE navigation channel.\"\n",
    "    )\n",
    "\n",
    "    # Adding attributes to data variables\n",
    "    ds3.depth.attrs[\"standard_name\"] = \"depth\"\n",
    "    ds3.depth.attrs[\"long_name\"] = \"depth below datum\"\n",
    "    ds3.depth.attrs[\"units\"] = \"US_survey_feet\"\n",
    "\n",
    "    ds3.time.attrs[\"standard_name\"] = \"time\"\n",
    "    ds3.time.attrs[\"long_name\"] = \"time\"\n",
    "    ds3.time.attrs[\"units\"] = time_units\n",
    "    ds3.time.encoding[\"units\"] = time_units\n",
    "\n",
    "    ds3.x.attrs[\"axis\"] = \"X\"  # Optional\n",
    "    ds3.x.attrs[\"standard_name\"] = \"projection_x_coordinate\"\n",
    "    ds3.x.attrs[\"long_name\"] = \"x-coordinate in projected coordinate system\"\n",
    "\n",
    "    ds3.y.attrs[\"axis\"] = \"Y\"  # Optional\n",
    "    ds3.y.attrs[\"standard_name\"] = \"projection_y_coordinate\"\n",
    "    ds3.y.attrs[\"long_name\"] = \"y-coordinate in projected coordinate system\"\n",
    "\n",
    "    # Query NCF REST API for ReachID projection information\n",
    "    sample_gdf = get_reach_properties(reach_ID=f\"{district5}_{reachID}\", returnGeo=True)\n",
    "\n",
    "    ds3 = ds3.rio.write_crs(input_crs=sample_gdf.sourceprojection[0])\n",
    "\n",
    "    comp = 5  # compression level\n",
    "    encoding_dict = {\n",
    "        \"depth\": {\n",
    "            \"zlib\": True,\n",
    "            \"complevel\": comp,\n",
    "        },\n",
    "        \"y\": {\n",
    "            \"zlib\": True,\n",
    "            \"complevel\": comp,\n",
    "        },\n",
    "        \"x\": {\n",
    "            \"zlib\": True,\n",
    "            \"complevel\": comp,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_filename = Path(rf\"../../Desktop/Data_prep_SWP/data/updated/{district5}/{reachID}.nc\")  # TODO: Update path to match your directory structure\n",
    "    ds3.to_netcdf(output_filename, encoding=encoding_dict)\n",
    "\n",
    "\n",
    "# Convert NetCDF files from 2D to 3D\n",
    "district5 = \"CEMVN\"\n",
    "reach_list = [p.stem for p in Path(rf\"../../Desktop/Data_prep_SWP/data/original/{district5}\").glob(\"SW_*\")]  # TODO: Update path to match your directory structure\n",
    "\n",
    "for r in reach_list:\n",
    "    clean_csat_netcdf(district5, r)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def remove_typesize_from_zmetadata(zarr_store_path):\n",
    "    zmeta_file = os.path.join(zarr_store_path, \".zmetadata\")\n",
    "\n",
    "    if not os.path.exists(zmeta_file):\n",
    "        print(f\"No .zmetadata file found at {zmeta_file}\")\n",
    "        return\n",
    "\n",
    "    with open(zmeta_file, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    # Recursively remove all \"typesize\" keys\n",
    "    def remove_typesize(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: remove_typesize(v) for k, v in obj.items() if k != \"typesize\"}\n",
    "        elif isinstance(obj, list):\n",
    "            return [remove_typesize(i) for i in obj]\n",
    "        return obj\n",
    "\n",
    "    cleaned_meta = remove_typesize(meta)\n",
    "\n",
    "    with open(zmeta_file, \"w\") as f:\n",
    "        json.dump(cleaned_meta, f, indent=2)\n",
    "\n",
    "    print(f\"Removed 'typesize' from .zmetadata in: {zarr_store_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW_01_SWP_01\n",
      "\tOriginal dates: 637\n",
      "\tReduced dates: 542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_01_SWP_01_2025-06-23.zarr\n",
      "SW_02_SWP_01\n",
      "\tOriginal dates: 634\n",
      "\tReduced dates: 616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_02_SWP_01_2025-06-23.zarr\n",
      "SW_03_SWP_01\n",
      "\tOriginal dates: 1793\n",
      "\tReduced dates: 1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_03_SWP_01_2025-06-23.zarr\n",
      "SW_04_SWP_01\n",
      "\tOriginal dates: 2376\n",
      "\tReduced dates: 2198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_04_SWP_01_2025-06-23.zarr\n",
      "SW_05_SWP_01\n",
      "\tOriginal dates: 2958\n",
      "\tReduced dates: 2475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_05_SWP_01_2025-06-23.zarr\n",
      "SW_06_SWP_01\n",
      "\tOriginal dates: 2037\n",
      "\tReduced dates: 1826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_06_SWP_01_2025-06-23.zarr\n",
      "SW_07_SWP_01\n",
      "\tOriginal dates: 1957\n",
      "\tReduced dates: 1732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_07_SWP_01_2025-06-23.zarr\n",
      "SW_08_SWP_01\n",
      "\tOriginal dates: 1815\n",
      "\tReduced dates: 1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_08_SWP_01_2025-06-23.zarr\n",
      "SW_09_SWP_01\n",
      "\tOriginal dates: 1908\n",
      "\tReduced dates: 1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_09_SWP_01_2025-06-23.zarr\n",
      "SW_10_SWP_01\n",
      "\tOriginal dates: 1851\n",
      "\tReduced dates: 1633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_10_SWP_01_2025-06-23.zarr\n",
      "SW_11_SWP_01\n",
      "\tOriginal dates: 1771\n",
      "\tReduced dates: 1560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_11_SWP_01_2025-06-23.zarr\n",
      "SW_12_SWP_01\n",
      "\tOriginal dates: 1678\n",
      "\tReduced dates: 1429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"x\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSaving Zarr file\n",
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_12_SWP_01_2025-06-23.zarr\n",
      "SW_13_SWP_01\n",
      "\tOriginal dates: 322\n",
      "\tReduced dates: 203\n",
      "\tSaving Zarr file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbroders\\AppData\\Local\\Temp\\ipykernel_43380\\3340438995.py:10: UserWarning: The specified chunks separate the stored chunks along dimension \"y\" starting at index 256. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 'typesize' from .zmetadata in: ..\\..\\Desktop\\Data_prep_SWP\\data\\interpolated\\CEMVN\\SW_13_SWP_01_2025-06-23.zarr\n"
     ]
    }
   ],
   "source": [
    "# Convert from NetCDF to Zarr format\n",
    "district5 = \"CEMVN\"\n",
    "reachID_list = [f\"SW_{x + 1:02}_SWP_01\" for x in range(0, 13)]  # TODO: Update to match your range of reach IDs\n",
    "\n",
    "for reachID in reachID_list:\n",
    "    print(reachID)\n",
    "\n",
    "    # Load SWP netcdf file\n",
    "    single_nc = Path(rf\"../../Desktop/Data_prep_SWP/data/updated/{district5}/{reachID}.nc\")  # TODO: Update path to match your directory structure\n",
    "    ds = xr.open_dataset(single_nc, chunks={\"y\": 256, \"x\": 256})  # specify initial chunks\n",
    "\n",
    "    # Demonstrate existance of duplicate dates\n",
    "    t = ds.time.data\n",
    "    print(f\"\\tOriginal dates: {len(t)}\")\n",
    "    print(f\"\\tReduced dates: {len(np.unique(t))}\")\n",
    "\n",
    "    # Merge duplicate dates\n",
    "    da = ds.depth.groupby(\"time\").reduce(np.nanmean, keep_attrs=True)\n",
    "    ds.close()\n",
    "    # rechunk the data for better timeseries analysis\n",
    "    da = da.chunk(dict(time=-1, x=100, y=100))\n",
    "\n",
    "    # fill NaN values (interpolation will error if NaNs are present)\n",
    "    da2 = da.interpolate_na(dim=\"time\", method=\"linear\")\n",
    "\n",
    "    # interpolate to a daily time series\n",
    "    da_fullrange = da2.interp(time=pd.date_range(da2.time.min().values, da2.time.max().values))\n",
    "\n",
    "    # --- Prepare output for storage ---\n",
    "\n",
    "    # convert from DataArray to Dataset\n",
    "    ds_fullrange = da_fullrange.to_dataset()\n",
    "\n",
    "    # copy original attributes for dataset and variables\n",
    "    ds_fullrange.attrs = ds.attrs\n",
    "    ds_fullrange[\"spatial_ref\"] = ds[\"spatial_ref\"]\n",
    "    ds_fullrange[\"time\"].attrs = ds[\"time\"].attrs\n",
    "\n",
    "    # set output path for zarr folder\n",
    "    output_dir = Path(rf\"../../Desktop/Data_prep_SWP/data/interpolated/{district5}/{reachID}_{datetime.today().strftime('%Y-%m-%d')}.zarr\")  # TODO: Update path to match your directory structure\n",
    "    # write to disk (Zarr)\n",
    "    print(\"\\tSaving Zarr file\")\n",
    "    ds_fullrange.to_zarr(\n",
    "    store=output_dir,\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    zarr_format=2\n",
    "   )\n",
    "    \n",
    "    remove_typesize_from_zmetadata(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last date for SW_01_SWP_01: 2025-05-27\n",
      "Last date for SW_02_SWP_01: 2025-05-28\n",
      "Last date for SW_03_SWP_01: 2025-05-28\n",
      "Last date for SW_04_SWP_01: 2025-05-29\n",
      "Last date for SW_05_SWP_01: 2025-06-02\n",
      "Last date for SW_06_SWP_01: 2025-05-29\n",
      "Last date for SW_07_SWP_01: 2025-06-02\n",
      "Last date for SW_08_SWP_01: 2025-06-02\n",
      "Last date for SW_09_SWP_01: 2025-05-30\n",
      "Last date for SW_10_SWP_01: 2025-06-01\n",
      "Last date for SW_11_SWP_01: 2025-06-02\n",
      "Last date for SW_12_SWP_01: 2025-05-28\n",
      "Last date for SW_13_SWP_01: 2025-05-23\n",
      "CSV saved\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "reachID_list = [f\"SW_{x + 1:02}_SWP_01\" for x in range(0, 13)]\n",
    "\n",
    "results = []\n",
    "\n",
    "for reach in reachID_list:\n",
    "    try:\n",
    "        zarr_path = rf\"../../Desktop/Data_prep_SWP/data/interpolated/CEMVN/{reach}_{datetime.today().strftime('%Y-%m-%d')}.zarr\"\n",
    "        zarr_array = xr.open_zarr(zarr_path)\n",
    "        last_date = zarr_array['time'][-1].values\n",
    "        last_date = pd.to_datetime(last_date).date()\n",
    "        results.append((reach, last_date))\n",
    "        print(f\"Last date for {reach}: {last_date}\")\n",
    "        zarr_array.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {reach}: {e}\")\n",
    "        results.append((reach, None))  # Log as missing\n",
    "\n",
    "# Create and save DataFrame\n",
    "df = pd.DataFrame(results, columns=[\"ReachID\", \"LastSurveyDate\"])\n",
    "df.to_csv(f\"reach_last_survey_dates_{datetime.today().strftime('%Y-%m-%d')}.csv\", index=False)\n",
    "print(\"CSV saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def dredge_percent_collection(depth,date):\\n    dredge_percent=[]\\n    threshold_deepening = 50\\n    threshold_pre_deepening = 45\\n    combined= [(pos, time) for pos, time in zip(depth, date)]\\n    for item in combined:\\n        if item[1]< np.datetime64('2020-01-01'):\\n            dredge = (item[0]< threshold_pre_deepening).sum()/(~np.isnan(item[0])).sum()\\n            dredge_percent.append([dredge,item[1]])\\n        else:\\n            dredge = (item[0]< threshold_deepening).sum()/(~np.isnan(item[0])).sum()\\n            dredge_percent.append([dredge,item[1]])\\n    return dredge_percent\\n    \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def dredge_percent_collection(depth,date):\n",
    "    dredge_percent=[]\n",
    "    threshold_deepening = 50\n",
    "    threshold_pre_deepening = 45\n",
    "    combined= [(pos, time) for pos, time in zip(depth, date)]\n",
    "    for item in combined:\n",
    "        if item[1]< np.datetime64('2020-01-01'):\n",
    "            dredge = (item[0]< threshold_pre_deepening).sum()/(~np.isnan(item[0])).sum()\n",
    "            dredge_percent.append([dredge,item[1]])\n",
    "        else:\n",
    "            dredge = (item[0]< threshold_deepening).sum()/(~np.isnan(item[0])).sum()\n",
    "            dredge_percent.append([dredge,item[1]])\n",
    "    return dredge_percent\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(10, 6))\\nplt.boxplot(box_data, vert=False, positions=range(1, len(box_data) + 1),showfliers=False)\\nplt.yticks(ticks=range(1, len(labels) + 1), labels=labels)\\nplt.xlabel(\"Percent Above Threshold\")\\nplt.title(\"Percent Above by Reach\")\\nplt.tight_layout()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(box_data, vert=False, positions=range(1, len(box_data) + 1),showfliers=False)\n",
    "plt.yticks(ticks=range(1, len(labels) + 1), labels=labels)\n",
    "plt.xlabel(\"Percent Above Threshold\")\n",
    "plt.title(\"Percent Above by Reach\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Day before dredge percent\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
